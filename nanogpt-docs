Subscription Usage
​
Overview

Returns subscription status and current daily/monthly usage for the active billing period.
​
Request

Method: GET
Path: /api/subscription/v1/usage
Auth: Authorization: Bearer <api_key> or x-api-key: <api_key>
​
Response

200 application/json. Timestamps are UNIX epoch milliseconds.
{
  "active": true,
  "limits": { "daily": 2000, "monthly": 60000 },
  "enforceDailyLimit": false,
  "daily": {
    "used": 5,
    "remaining": 1995,
    "percentUsed": 0.0025,
    "resetAt": 1738540800000
  },
  "monthly": {
    "used": 45,
    "remaining": 59955,
    "percentUsed": 0.00075,
    "resetAt": 1739404800000
  },
  "period": {
    "currentPeriodEnd": "2025-02-13T23:59:59.000Z"
  },
  "state": "active",
  "graceUntil": null
}
Fields
active — Whether the account is currently active for subscription usage.
limits.daily, limits.monthly — Configured daily/monthly allowance.
enforceDailyLimit — If true, access requires both daily AND monthly remaining > 0; if false, only monthly remaining is required.
daily.used, monthly.used — Usage units consumed in the current day/month window.
daily.remaining, monthly.remaining — Remaining allowance for each window.
daily.percentUsed, monthly.percentUsed — Decimal fraction in [0,1].
daily.resetAt, monthly.resetAt — Millisecond epoch when the window resets.
period.currentPeriodEnd — ISO timestamp for the end of the current billing period, if known.
state — One of active, grace, inactive.
graceUntil — ISO timestamp when grace access ends (if applicable).
​
Usage semantics

Usage units represent successful subscription‑covered operations (e.g., a completed generation). They are not tokens or dollar cost.
Daily window resets at the next UTC day start; monthly usage aligns to the subscription billing cycle when available.
​
Examples


cURL

JavaScript/TypeScript
curl -s \
  -H "Authorization: Bearer $NANOGPT_API_KEY" \
  https://nano-gpt.com/api/subscription/v1/usage | jq



## GET ALL SUBSCRIPTION MODELS
GET /api/subscription/v1/models (subscription-only)

Always returns only models included in the NanoGPT subscription (equivalent to our isTextEligible(modelId) filter).
Ignores the user’s “Also show paid models” preference; it is always subscription-only.
Supports ?detailed=true and API key–aware, at-cost pricing metadata.
Examples:
curl https://nano-gpt.com/api/subscription/v1/models
curl -H "x-api-key: $NANOGPT_API_KEY" \
  "https://nano-gpt.com/api/subscription/v1/models?detailed=true"


Endpoint Examples
Chat Completion
POST
/
v1
/
chat
/
completions

Try it
​
Tool calling

The /api/v1/chat/completions endpoint supports OpenAI-compatible function calling. You can describe callable functions in the tools array, control when the model may invoke them, and continue the conversation by echoing tool role messages that reference the assistant’s chosen call.
​
Request parameters

tools (optional array): Each entry must be { "type": "function", "function": { "name": string, "description"?: string, "parameters"?: JSON-Schema object } }. Only function tools are accepted. The serialized tools payload is limited to 200 KB (overrides via TOOL_SPEC_MAX_BYTES); violating the shape or size yields a 400 with tool_spec_too_large, invalid_tool_spec, or invalid_tool_spec_parse.
tool_choice (optional string or object): Defaults to auto. Set "none" to guarantee no tool calls (the server also drops the tools payload upstream), "required" to force the next response to be a tool call, or { "type": "function", "function": { "name": "your_function" } } to pin the exact function.
parallel_tool_calls (optional boolean): When true the flag is forwarded to providers that support issuing multiple tool calls in a single turn. Models that ignore the flag fall back to sequential calls.
messages[].tool_calls (assistant role): Persist the tool call metadata returned by the model so future turns can see which functions were invoked. Each item uses the OpenAI shape { id, type: "function", function: { name, arguments } }.
messages[] with role: "tool": Respond to the model by sending { "role": "tool", "tool_call_id": "<assistant tool_calls id>", "content": "<JSON or text payload>" }. The server drops any tool response that references an unknown tool_call_id, so keep the IDs in sync.
Validation behavior: If you send tool_choice: "none" with a tools array the request is accepted but the tools are omitted before hitting the model; invalid schemas or oversize payloads return the error codes above.
​
Example request

POST /api/v1/chat/completions
{
  "model": "gpt-4o-mini",
  "messages": [
    { "role": "user", "content": "What's the temperature in San Francisco right now?" }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "lookup_weather",
        "description": "Fetch the current weather for a city.",
        "parameters": {
          "type": "object",
          "properties": {
            "city": { "type": "string" },
            "unit": { "type": "string", "enum": ["c", "f"] }
          },
          "required": ["city"]
        }
      }
    }
  ],
  "tool_choice": "auto",
  "parallel_tool_calls": true
}
​
Example assistant/tool turn

{
  "role": "assistant",
  "content": null,
  "tool_calls": [
    {
      "id": "call_abc123",
      "type": "function",
      "function": {
        "name": "lookup_weather",
        "arguments": "{\"city\":\"San Francisco\",\"unit\":\"f\"}"
      }
    }
  ]
}
{
  "role": "tool",
  "tool_call_id": "call_abc123",
  "content": "{\"city\":\"San Francisco\",\"temperatureF\":58,\"conditions\":\"foggy\"}"
}
Streaming responses emit delta events that mirror OpenAI’s tool_calls schema, so consumers can reuse their existing parsing logic without changes.
​
Overview

The Chat Completion endpoint provides OpenAI-compatible chat completions.
​
Sampling & Decoding Controls

The /api/v1/chat/completions endpoint accepts a full set of sampling and decoding knobs. All fields are optional; omit any you want to leave at provider defaults.
​
Temperature & Nucleus

Parameter	Range/Default	Description
temperature	0–2 (default 0.7)	Classic randomness control; higher values explore more.
top_p	0–1 (default 1)	Nucleus sampling that trims to the smallest set above top_p cumulative probability.
top_k	1+	Sample only from the top-k tokens each step.
top_a	provider default	Blends temperature and nucleus behavior; set only if a model calls for it.
min_p	0–1	Require each candidate token to exceed a probability floor.
tfs	0–1	Tail free sampling; 1 disables.
eta_cutoff / epsilon_cutoff	provider default	Drop tokens once they fall below the tail thresholds.
typical_p	0–1	Entropy-based nucleus sampling; keeps tokens whose surprise matches expected entropy.
mirostat_mode	0/1/2	Enable Mirostat sampling; set tau/eta when active.
mirostat_tau / mirostat_eta	provider default	Target entropy and learning rate for Mirostat.
​
Length & Stopping

Parameter	Range/Default	Description
max_tokens	1+ (default 4000)	Upper bound on generated tokens.
min_tokens	0+ (default 0)	Minimum completion length when provider supports it.
stop	string or string[]	Stop sequences passed upstream.
stop_token_ids	int[]	Stop generation on specific token IDs (limited provider support).
include_stop_str_in_output	boolean (default false)	Keep the stop sequence in the final text where supported.
ignore_eos	boolean (default false)	Continue even if the model predicts EOS internally.
​
Penalties & Repetition Guards

Parameter	Range/Default	Description
frequency_penalty	-2 – 2 (default 0)	Penalize tokens proportional to prior frequency.
presence_penalty	-2 – 2 (default 0)	Penalize tokens based on whether they appeared at all.
repetition_penalty	-2 – 2	Provider-agnostic repetition modifier; >1 discourages repeats.
no_repeat_ngram_size	0+	Forbid repeating n-grams of the given size (limited support).
custom_token_bans	int[]	Fully block listed token IDs.
​
Logit Shaping & Determinism

Parameter	Range/Default	Description
logit_bias	object	Map token IDs to additive logits (OpenAI-compatible).
logprobs	boolean or int	Return token-level logprobs where supported.
prompt_logprobs	boolean	Request logprobs on the prompt when available.
seed	integer	Make completions repeatable where the provider allows it.
​
Usage notes

Parameters can be combined (e.g., temperature + top_p + top_k), but overly narrow settings may lead to early stops.
Invalid ranges yield a 400 before reaching the provider.
Provider defaults apply to any omitted field.
​
Example request

curl -X POST https://nano-gpt.com/api/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Write a creative story about space exploration"}
    ],
    "temperature": 0.8,
    "top_p": 0.9,
    "top_k": 40,
    "tfs": 0.8,
    "typical_p": 0.95,
    "mirostat_mode": 2,
    "mirostat_tau": 5,
    "mirostat_eta": 0.1,
    "max_tokens": 500,
    "frequency_penalty": 0.3,
    "presence_penalty": 0.1,
    "repetition_penalty": 1.1,
    "stop": ["###"],
    "seed": 42
  }'
  Message Shape

{
  "role": "user",
  "content": [
    { "type": "text", "text": "What is in this image?" },
    { "type": "image_url", "image_url": { "url": "https://example.com/image.jpg" } }
  ]
}
​
cURL — Image URL (non‑streaming)

curl -sS \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST https://nano-gpt.com/api/v1/chat/completions \
  --data '{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "Describe this image in three words."},
          {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/3/3f/Fronalpstock_big.jpg"}}
        ]
      }
    ],
    "stream": false
  }'
​
cURL — Base64 Data URL (non‑streaming)

Embed your image as a data URL. Replace ...BASE64... with your image bytes.
curl -sS \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type": "application/json" \
  -X POST https://nano-gpt.com/api/v1/chat/completions \
  --data '{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "What is shown here?"},
          {"type": "image_url", "image_url": {"url": "data:image/png;base64,...BASE64..."}}
        ]
      }
    ],
    "stream": false
  }'
​
cURL — Streaming SSE

curl -N \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -X POST https://nano-gpt.com/api/v1/chat/completions \
  --data '{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "Two words only."},
          {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
        ]
      }
    ],
    "stream": true,
    "stream_options": { "include_usage": true }
  }'
The response streams data: { ... } lines until a final terminator. Usage metrics appear only when requested: set stream_options.include_usage to true for streaming responses, or send "include_usage": true on non-streaming calls.
Note: Prompt-caching helpers implicitly force include_usage, so cached requests still receive usage data without extra flags.

Reasoning Streams

The Chat Completions endpoint separates the model’s visible answer from its internal reasoning. By default, reasoning is included and delivered alongside normal content so that clients can decide whether to display it. Requests that use the thinking model suffix (for example :thinking or -thinking:8192) are normalized before dispatch, but the response contract remains the same.
​
Endpoint variants

Choose the base path that matches how your client consumes reasoning streams:
https://nano-gpt.com/api/v1/chat/completions — default endpoint that streams internal thoughts through choices[0].delta.reasoning (and repeats them in message.reasoning on completion). Recommended for apps like SillyTavern that understand the modern response shape.
https://nano-gpt.com/api/v1legacy/chat/completions — legacy contract that swaps the field name to choices[0].delta.reasoning_content / message.reasoning_content for older OpenAI-compatible clients. Use this for LiteLLM’s OpenAI adapter to avoid downstream parsing errors.
https://nano-gpt.com/api/v1thinking/chat/completions — reasoning-aware models write everything into the normal choices[0].delta.content stream so clients that ignore reasoning fields still see the full conversation transcript. This is the preferred base URL for JanitorAI.
​
Streaming payload format

Server-Sent Event (SSE) streams emit the answer in choices[0].delta.content and the thought process in choices[0].delta.reasoning (plus optional delta.reasoning_details). Reasoning deltas are dispatched before or alongside regular content, letting you render both panes in real-time.
data: {
  "choices": [{
    "delta": {
      "reasoning": "Assessing possible tool options…"
    }
  }]
}
data: {
  "choices": [{
    "delta": {
      "content": "Let me walk you through the solution."
    }
  }]
}
When streaming completes, the formatter aggregates the collected values and repeats them in the final payload: choices[0].message.content contains the assistant reply and choices[0].message.reasoning (plus reasoning_details when available) contains the full chain-of-thought. Non-streaming requests reuse the same formatter, so the reasoning block is present as a dedicated field.
​
Showing or hiding reasoning

Send reasoning: { "exclude": true } to strip the reasoning payload from both streaming deltas and the final message. With this flag set, delta.reasoning and message.reasoning are omitted entirely.
curl -X POST https://nano-gpt.com/api/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-5-sonnet-20241022",
    "messages": [{"role": "user", "content": "What is 2+2?"}],
    "reasoning": {"exclude": true}
  }'
Without reasoning.exclude:
{
  "choices": [{
    "message": {
      "content": "The answer is 4.",
      "reasoning": "The user is asking for a simple addition. 2+2 equals 4."
    }
  }]
}
With reasoning.exclude:
{
  "choices": [{
    "message": {
      "content": "The answer is 4."
    }
  }]
}
​
Reasoning Effort

Control how much computational effort the model puts into reasoning before generating a response. Higher values result in more thorough reasoning but slower responses and higher costs. Only applicable to reasoning-capable models.
​
Parameter: reasoning_effort

Value	Description
none	Disables reasoning entirely
minimal	Allocates ~10% of max_tokens for reasoning
low	Allocates ~20% of max_tokens for reasoning
medium	Allocates ~50% of max_tokens for reasoning (default when reasoning is enabled)
high	Allocates ~80% of max_tokens for reasoning
​
Usage

The reasoning_effort parameter can be passed at the top level:
curl -X POST https://nano-gpt.com/api/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "o3-mini",
    "messages": [
      {"role": "user", "content": "Explain quantum entanglement step by step"}
    ],
    "reasoning_effort": "high",
    "max_tokens": 4096
  }'
Alternatively, pass it as part of the reasoning object:
{
  "model": "o3-mini",
  "messages": [{"role": "user", "content": "Solve this complex math problem..."}],
  "reasoning": {
    "effort": "high"
  }
}
Both formats are equivalent.
​
Combining effort with exclude

You can control both reasoning depth and visibility:
{
  "model": "deepseek-reasoner",
  "messages": [{"role": "user", "content": "..."}],
  "reasoning": {
    "effort": "high",
    "exclude": false
  }
}
Sending reasoning_effort to models that don’t support reasoning will have no effect (the parameter is ignored).
​
Model suffix: :reasoning-exclude

You can toggle the filter without altering your JSON body by appending :reasoning-exclude to the model name.
Equivalent to sending { "reasoning": { "exclude": true } }
Only the :reasoning-exclude suffix is stripped before the request is routed; other suffixes remain active
Works for streaming and non-streaming responses on both Chat Completions and Text Completions
{
  "model": "claude-3-5-sonnet-20241022:reasoning-exclude",
  "messages": [{ "role": "user", "content": "What is 2+2?" }]
}
​
Combine with other suffixes

:reasoning-exclude composes safely with the other routing suffixes you already use:
:thinking (and variants like …-thinking:8192)
:online and :online/linkup-deep
:memory and :memory-<days>
Examples:
claude-3-7-sonnet-thinking:8192:reasoning-exclude
gpt-4o:online:reasoning-exclude
claude-3-5-sonnet-20241022:memory-30:online/linkup-deep:reasoning-exclude

JavaScript Library
Node.js library for interacting with NanoGPT API
​
NanoGPTJS

NanoGPTJS is a Node.js library designed to interact with NanoGPT’s API. This library provides an easy way to integrate NanoGPT’s capabilities into your JavaScript applications.
​
Overview

The NanoGPT service enables pay-per-prompt interaction with chat and image generation models. You will need a prefilled NanoGPT wallet and API key to use this library effectively.
​
Installation

You can install the library using npm:
npm install nanogptjs
​
Basic Usage

const NanoGPT = require('nanogptjs');

// Initialize with your API key
const nanogpt = new NanoGPT('your-api-key');

async function chatExample() {
  try {
    const response = await nanogpt.chat({
      model: 'chatgpt-4o-latest',
      messages: [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: 'What is the capital of France?' }
      ]
    });
    
    console.log(response);
  } catch (error) {
    console.error('Error:', error);
  }
}

chatExample();
​
Features

Chat Completions: Generate text responses using various AI models
Image Generation: Create images from text prompts
Model Selection: Choose from a wide range of available models
Balance Management: Check your NanoGPT balance and manage transactions
​
API Methods

​
Chat

const response = await nanogpt.chat({
  model: 'chatgpt-4o-latest',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Hello, how are you?' }
  ],
  temperature: 0.7,
  max_tokens: 150
});
​
Generate Image

const response = await nanogpt.generateImage({
  prompt: 'A beautiful sunset over the ocean',
  model: 'recraft-v3',
  width: 1024,
  height: 1024
});
​
Check Balance

const balance = await nanogpt.checkBalance();
console.log('USD Balance:', balance.usd_balance);
console.log('Nano Balance:', balance.nano_balance);
​
Error Handling

The library provides robust error handling to manage API response errors:
try {
  const response = await nanogpt.chat({
    model: 'chatgpt-4o-latest',
    messages: [
      { role: 'user', content: 'Hello!' }
    ]
  });
} catch (error) {
  console.error('Status:', error.status);
  console.error('Message:', error.message);
}


